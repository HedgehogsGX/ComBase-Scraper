#!/usr/bin/env python3
"""
å¹¶è¡ŒComBaseçˆ¬è™« - 10ä¸ªçº¿ç¨‹åŒæ—¶çˆ¬å–ä¸åŒé¡µé¢
"""
import sys
import os
import time
import json
import threading
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from scrapers.main_scraper import ComBaseMainScraper

class ParallelComBaseScraper:
    def __init__(self, total_pages=6075, num_threads=10, records_per_file=1000):
        self.total_pages = total_pages
        self.num_threads = num_threads
        self.records_per_file = records_per_file
        self.output_dir = Path("data")
        self.output_dir.mkdir(exist_ok=True)
        
        # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦è·Ÿè¸ª
        self.progress_lock = threading.Lock()
        self.completed_pages = 0
        self.total_records = 0
        self.active_threads = 0
        self.start_time = time.time()
        
        # åˆ†é…é¡µé¢èŒƒå›´ç»™æ¯ä¸ªçº¿ç¨‹
        self.page_ranges = self.distribute_pages()
        
    def distribute_pages(self):
        """å°†é¡µé¢åˆ†é…ç»™ä¸åŒçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸åŒçš„é¡µé¢èŒƒå›´"""
        pages_per_thread = self.total_pages // self.num_threads
        ranges = []
        
        for i in range(self.num_threads):
            start_page = i * pages_per_thread + 1
            if i == self.num_threads - 1:  # æœ€åä¸€ä¸ªçº¿ç¨‹å¤„ç†å‰©ä½™é¡µé¢
                end_page = self.total_pages
            else:
                end_page = (i + 1) * pages_per_thread
            
            ranges.append((start_page, end_page, i))
        
        return ranges
    
    def clear_screen(self):
        """æ¸…å±"""
        os.system('clear' if os.name == 'posix' else 'cls')
    
    def format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def draw_progress_bar(self, current, total, width=50):
        """ç»˜åˆ¶è¿›åº¦æ¡"""
        progress = current / total if total > 0 else 0
        filled = int(width * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        percentage = progress * 100
        return f"|{bar}| {percentage:.1f}%"
    
    def display_progress(self):
        """æ˜¾ç¤ºæ€»ä½“è¿›åº¦"""
        with self.progress_lock:
            self.clear_screen()
            
            elapsed = time.time() - self.start_time
            progress_percent = (self.completed_pages / self.total_pages) * 100
            
            print("ğŸš€ Parallel ComBase Scraper Progress")
            print("=" * 70)
            print(f"â° Runtime: {self.format_time(elapsed)}")
            print(f"ğŸ“„ Completed Pages: {self.completed_pages:,} / {self.total_pages:,}")
            print(f"ğŸ“Š Completion: {progress_percent:.2f}%")
            print(f"ğŸ”„ Active Threads: {self.active_threads}/{self.num_threads}")
            print()
            
            # è¿›åº¦æ¡
            progress_bar = self.draw_progress_bar(self.completed_pages, self.total_pages)
            print(f"ğŸ“ˆ Progress: {progress_bar}")
            print()
            
            print(f"ğŸ“ Total Records: {self.total_records:,}")
            print(f"ğŸ“ Records per File: {self.records_per_file:,}")
            print()
            
            # é€Ÿåº¦å’Œæ—¶é—´ä¼°ç®—
            if elapsed > 0 and self.completed_pages > 0:
                pages_per_hour = (self.completed_pages / elapsed) * 3600
                remaining_pages = self.total_pages - self.completed_pages
                if pages_per_hour > 0:
                    remaining_seconds = remaining_pages / (pages_per_hour / 3600)
                    print(f"âš¡ Speed: {pages_per_hour:.1f} pages/hour")
                    print(f"â±ï¸ ETA: {self.format_time(remaining_seconds)}")
            
            print()
            print("ğŸ’¡ Press Ctrl+C to stop all threads safely")
            print("=" * 70)
    
    def worker_thread(self, start_page, end_page, thread_id):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        try:
            with self.progress_lock:
                self.active_threads += 1
            
            print(f"ğŸ”§ Thread {thread_id}: Starting pages {start_page}-{end_page}")
            
            # åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹
            thread_output_dir = self.output_dir / f"thread_{thread_id}"
            scraper = ComBaseMainScraper(output_dir=str(thread_output_dir))
            scraper.records_per_file = self.records_per_file
            
            # é‡å†™è¿›åº¦å›è°ƒ
            def update_progress(page_num):
                with self.progress_lock:
                    self.completed_pages += 1
                    # ä¼°ç®—è®°å½•æ•°ï¼ˆå‡è®¾æ¯é¡µ10æ¡è®°å½•ï¼‰
                    self.total_records += 10
                self.display_progress()
            
            # å¼€å§‹çˆ¬å–
            scraper.run_main_scraping(
                username="WallaceGuo@moonshotacademy.cn",
                password="Rr*Auzqv!b9!Cnh",
                start_page=start_page,
                end_page=end_page,
                progress_callback=update_progress,
                search_delay=120  # æœç´¢åç­‰å¾…2åˆ†é’Ÿ
            )
            
            print(f"âœ… Thread {thread_id}: Completed pages {start_page}-{end_page}")
            
        except Exception as e:
            print(f"âŒ Thread {thread_id}: Error - {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            with self.progress_lock:
                self.active_threads -= 1
    
    def run(self):
        """è¿è¡Œå¹¶è¡Œçˆ¬è™«"""
        print("ğŸ¯ Starting Parallel ComBase Scraper")
        print("=" * 70)
        print("Configuration:")
        print(f"  - Total Pages: {self.total_pages:,}")
        print(f"  - Threads: {self.num_threads}")
        print(f"  - Records per File: {self.records_per_file:,}")
        print(f"  - Search Delay: 2 minutes")
        print("  - Using organism deduplication")
        print("=" * 70)
        
        # æ˜¾ç¤ºé¡µé¢åˆ†é…
        print("\nğŸ“‹ Page Distribution:")
        for start, end, thread_id in self.page_ranges:
            pages_count = end - start + 1
            print(f"  Thread {thread_id}: Pages {start:,}-{end:,} ({pages_count:,} pages)")
        print()
        
        try:
            # æ˜¾ç¤ºåˆå§‹è¿›åº¦
            self.display_progress()
            
            # å¯åŠ¨çº¿ç¨‹æ± 
            with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = []
                for start_page, end_page, thread_id in self.page_ranges:
                    future = executor.submit(self.worker_thread, start_page, end_page, thread_id)
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"âŒ Thread failed: {e}")
            
            # æœ€ç»ˆæ˜¾ç¤º
            self.display_progress()
            print("\nğŸ‰ All threads completed!")
            
        except KeyboardInterrupt:
            self.display_progress()
            print("\n\nâ¸ï¸ Scraping interrupted by user")
            print("ğŸ’¾ Saved data will not be lost")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            elapsed = time.time() - self.start_time
            print(f"\nğŸ“Š Final Statistics:")
            print(f"  - Runtime: {self.format_time(elapsed)}")
            print(f"  - Pages Completed: {self.completed_pages:,} / {self.total_pages:,}")
            print(f"  - Total Records: {self.total_records:,}")
            print(f"  - Completion: {(self.completed_pages / self.total_pages) * 100:.2f}%")
            
        except Exception as e:
            self.display_progress()
            print(f"\nâŒ Error during parallel scraping: {e}")
            import traceback
            traceback.print_exc()

def main():
    # é…ç½®å‚æ•°
    TOTAL_PAGES = 6075
    NUM_THREADS = 10
    RECORDS_PER_FILE = 1000
    
    scraper = ParallelComBaseScraper(
        total_pages=TOTAL_PAGES,
        num_threads=NUM_THREADS,
        records_per_file=RECORDS_PER_FILE
    )
    scraper.run()

if __name__ == "__main__":
    main()
