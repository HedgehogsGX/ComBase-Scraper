#!/usr/bin/env python3
"""
ComBaseæ•°æ®å¤„ç†å™¨
å¤„ç†çˆ¬å–çš„åŸå§‹æ•°æ®ï¼Œæå–å®Œæ•´ä¿¡æ¯å¹¶æ•´ç†å­˜å‚¨
"""

import os
import json
import csv
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
import re

class ComBaseDataProcessor:
    """ComBaseæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data", processed_dir: str = "data/processed"):
        self.data_dir = Path(data_dir)
        self.processed_dir = Path(processed_dir)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_records = 0
        self.duplicate_records = 0
        self.unique_records = 0
        
    def find_data_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        csv_files = []
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        patterns = [
            "combase_*.csv",
            "combase_pages_*.csv",
            "combase_batch_*.csv",
            "combase_complete_*.csv",
            "test_*.csv"
        ]
        
        for pattern in patterns:
            csv_files.extend(self.data_dir.glob(pattern))
        
        # æ’é™¤å·²å¤„ç†çš„æ–‡ä»¶
        csv_files = [f for f in csv_files if "processed" not in str(f)]
        
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for file in csv_files:
            print(f"  - {file.name}")
            
        return csv_files
    
    def load_and_merge_data(self, files: List[Path]) -> pd.DataFrame:
        """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        all_data = []
        
        for file in files:
            try:
                df = pd.read_csv(file)
                print(f"åŠ è½½ {file.name}: {len(df)} æ¡è®°å½•")
                all_data.append(df)
            except Exception as e:
                print(f"åŠ è½½ {file.name} å¤±è´¥: {e}")
                continue
        
        if not all_data:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_df = pd.concat(all_data, ignore_index=True)
        self.total_records = len(merged_df)
        
        print(f"\nğŸ“Š æ•°æ®åˆå¹¶ç»“æœ:")
        print(f"æ€»è®°å½•æ•°: {self.total_records}")
        
        return merged_df
    
    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """å»é™¤é‡å¤è®°å½•"""
        print("\nğŸ” æ£€æŸ¥é‡å¤è®°å½•...")
        
        # åŸºäºrecord_idå»é‡
        before_count = len(df)
        df_unique = df.drop_duplicates(subset=['record_id'], keep='first')
        after_count = len(df_unique)
        
        self.duplicate_records = before_count - after_count
        self.unique_records = after_count
        
        print(f"å»é‡å‰: {before_count} æ¡è®°å½•")
        print(f"å»é‡å: {after_count} æ¡è®°å½•")
        print(f"é‡å¤è®°å½•: {self.duplicate_records} æ¡")
        
        return df_unique
    
    def validate_data_consistency(self, df: pd.DataFrame):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        
        # æ£€æŸ¥é¡µé¢è®°å½•æ•°
        page_counts = df['page_number'].value_counts().sort_index()
        
        print("å„é¡µé¢è®°å½•æ•°:")
        inconsistent_pages = []
        for page, count in page_counts.items():
            status = "âœ“" if count == 10 else "âš ï¸"
            print(f"  ç¬¬ {page} é¡µ: {count} æ¡è®°å½• {status}")
            if count != 10:
                inconsistent_pages.append(page)
        
        if inconsistent_pages:
            print(f"âš ï¸ å‘ç° {len(inconsistent_pages)} ä¸ªé¡µé¢è®°å½•æ•°ä¸æ˜¯10æ¡")
            print(f"å¼‚å¸¸é¡µé¢: {inconsistent_pages}")
        else:
            print("âœ… æ‰€æœ‰é¡µé¢è®°å½•æ•°éƒ½æ­£ç¡®")
        
        # æ£€æŸ¥record_idè¿ç»­æ€§
        record_ids = df['record_id'].astype(int).sort_values()
        gaps = []
        for i in range(1, len(record_ids)):
            if record_ids.iloc[i] - record_ids.iloc[i-1] > 1:
                gaps.append((record_ids.iloc[i-1], record_ids.iloc[i]))
        
        if gaps:
            print(f"âš ï¸ å‘ç° {len(gaps)} ä¸ªrecord_idé—´éš™:")
            for start, end in gaps[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {start} -> {end}")
        else:
            print("âœ… record_idè¿ç»­æ— é—´éš™")
    
    def enhance_data_structure(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºæ•°æ®ç»“æ„ï¼Œæ·»åŠ æ›´å¤šå­—æ®µ"""
        print("\nğŸ”§ å¢å¼ºæ•°æ®ç»“æ„...")
        
        enhanced_df = df.copy()
        
        # 1. è§£æé£Ÿå“ä¿¡æ¯
        enhanced_df['food_category'] = enhanced_df['food'].apply(self.extract_food_category)
        enhanced_df['food_name'] = enhanced_df['food'].apply(self.extract_food_name)
        
        # 2. è§£ææ—¶é—´åºåˆ—ç»Ÿè®¡
        enhanced_df['logc_range'] = enhanced_df.apply(
            lambda row: row['logc_final'] - row['logc_initial'] if pd.notna(row['logc_final']) and pd.notna(row['logc_initial']) else None, 
            axis=1
        )
        
        enhanced_df['logc_max'] = enhanced_df['logc_series_json'].apply(self.extract_max_from_series)
        enhanced_df['logc_min'] = enhanced_df['logc_series_json'].apply(self.extract_min_from_series)
        
        # 3. æ·»åŠ æ•°æ®è´¨é‡æ ‡è®°
        enhanced_df['has_time_series'] = enhanced_df['logc_points'].notna() & (enhanced_df['logc_points'] > 0)
        enhanced_df['data_completeness'] = enhanced_df.apply(self.calculate_completeness, axis=1)
        
        # 4. æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        enhanced_df['processed_at'] = datetime.now().isoformat()
        
        print(f"âœ… æ•°æ®ç»“æ„å¢å¼ºå®Œæˆï¼Œæ–°å¢ {len(enhanced_df.columns) - len(df.columns)} ä¸ªå­—æ®µ")
        
        return enhanced_df
    
    def extract_food_category(self, food_text: str) -> str:
        """æå–é£Ÿå“ç±»åˆ«"""
        if pd.isna(food_text) or not food_text:
            return "Unknown"
        
        food_text = food_text.lower()
        
        # å®šä¹‰é£Ÿå“ç±»åˆ«æ˜ å°„
        categories = {
            'beef': ['beef', 'steak', 'striploin', 'cube roll'],
            'seafood': ['salmon', 'fish', 'oyster', 'sole', 'seafood'],
            'poultry': ['chicken', 'turkey', 'poultry'],
            'dairy': ['milk', 'cheese', 'yogurt', 'dairy'],
            'vegetable': ['vegetable', 'lettuce', 'carrot'],
            'processed_meat': ['precooked', 'smoked', 'cured']
        }
        
        for category, keywords in categories.items():
            if any(keyword in food_text for keyword in keywords):
                return category.replace('_', ' ').title()
        
        return "Other"
    
    def extract_food_name(self, food_text: str) -> str:
        """æå–é£Ÿå“åç§°"""
        if pd.isna(food_text) or not food_text:
            return "Unknown"
        
        # ç§»é™¤"in "å‰ç¼€
        if food_text.startswith("in "):
            return food_text[3:].strip()
        
        return food_text.strip()
    
    def extract_max_from_series(self, series_json: str) -> float:
        """ä»æ—¶é—´åºåˆ—ä¸­æå–æœ€å¤§å€¼"""
        try:
            if pd.isna(series_json):
                return None
            series_data = json.loads(series_json)
            values = [point[1] for point in series_data]
            return max(values) if values else None
        except:
            return None
    
    def extract_min_from_series(self, series_json: str) -> float:
        """ä»æ—¶é—´åºåˆ—ä¸­æå–æœ€å°å€¼"""
        try:
            if pd.isna(series_json):
                return None
            series_data = json.loads(series_json)
            values = [point[1] for point in series_data]
            return min(values) if values else None
        except:
            return None
    
    def calculate_completeness(self, row) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´åº¦"""
        total_fields = 8  # ä¸»è¦å­—æ®µæ•°é‡
        complete_fields = 0
        
        fields_to_check = ['organism', 'food', 'temperature', 'aw', 'ph', 'logc_points', 'logc_duration', 'logc_series_json']
        
        for field in fields_to_check:
            if field in row and pd.notna(row[field]) and str(row[field]).strip() not in ['', 'Not specified', 'None']:
                complete_fields += 1
        
        return complete_fields / total_fields
    
    def generate_summary_statistics(self, df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        stats = {
            'total_records': len(df),
            'unique_organisms': df['organism'].nunique(),
            'unique_foods': df['food_name'].nunique(),
            'food_categories': df['food_category'].value_counts().to_dict(),
            'temperature_range': {
                'min': df['temperature'].astype(str).str.extract(r'(\d+)').astype(float).min().iloc[0] if not df['temperature'].empty else None,
                'max': df['temperature'].astype(str).str.extract(r'(\d+)').astype(float).max().iloc[0] if not df['temperature'].empty else None
            },
            'time_series_stats': {
                'records_with_series': df['has_time_series'].sum(),
                'avg_data_points': df['logc_points'].mean(),
                'avg_duration_hours': df['logc_duration'].mean()
            },
            'data_quality': {
                'avg_completeness': df['data_completeness'].mean(),
                'high_quality_records': (df['data_completeness'] >= 0.8).sum()
            }
        }
        
        return stats
    
    def save_processed_data(self, df: pd.DataFrame, stats: Dict):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
        main_file = self.processed_dir / f"combase_processed_{timestamp}.csv"
        df.to_csv(main_file, index=False, encoding='utf-8')
        print(f"âœ… ä¸»æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {main_file}")
        
        # 2. ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        stats_file = self.processed_dir / f"combase_statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
        print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {stats_file}")
        
        # 3. ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆåªåŒ…å«ä¸»è¦å­—æ®µï¼‰
        essential_columns = [
            'record_id', 'organism', 'food_category', 'food_name', 
            'temperature', 'aw', 'ph', 'page_number',
            'logc_points', 'logc_duration', 'logc_initial', 'logc_final',
            'logc_min', 'logc_max', 'logc_range',
            'has_time_series', 'data_completeness'
        ]
        
        essential_df = df[essential_columns]
        essential_file = self.processed_dir / f"combase_essential_{timestamp}.csv"
        essential_df.to_csv(essential_file, index=False, encoding='utf-8')
        print(f"âœ… ç²¾ç®€æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {essential_file}")
        
        # 4. ç”Ÿæˆå¯è¯»çš„ç»Ÿè®¡æŠ¥å‘Š
        report_file = self.processed_dir / f"combase_report_{timestamp}.txt"
        self.generate_readable_report(stats, report_file)
        print(f"âœ… å¯è¯»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return main_file, essential_file, stats_file, report_file
    
    def generate_readable_report(self, stats: Dict, report_file: Path):
        """ç”Ÿæˆå¯è¯»çš„ç»Ÿè®¡æŠ¥å‘Š"""
        report = f"""
ComBaseæ•°æ®å¤„ç†æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š æ•°æ®æ¦‚è§ˆ:
- æ€»è®°å½•æ•°: {stats['total_records']:,}
- å”¯ä¸€å¾®ç”Ÿç‰©: {stats['unique_organisms']} ç§
- å”¯ä¸€é£Ÿå“: {stats['unique_foods']} ç§

ğŸ– é£Ÿå“ç±»åˆ«åˆ†å¸ƒ:
"""
        
        for category, count in stats['food_categories'].items():
            percentage = count / stats['total_records'] * 100
            report += f"- {category}: {count:,} æ¡ ({percentage:.1f}%)\n"
        
        report += f"""
ğŸŒ¡ï¸ æ¸©åº¦èŒƒå›´:
- æœ€ä½æ¸©åº¦: {stats['temperature_range']['min']}Â°C
- æœ€é«˜æ¸©åº¦: {stats['temperature_range']['max']}Â°C

ğŸ“ˆ æ—¶é—´åºåˆ—æ•°æ®:
- åŒ…å«æ—¶é—´åºåˆ—: {stats['time_series_stats']['records_with_series']:,} æ¡
- å¹³å‡æ•°æ®ç‚¹: {stats['time_series_stats']['avg_data_points']:.1f} ä¸ª
- å¹³å‡è§‚å¯Ÿæ—¶é•¿: {stats['time_series_stats']['avg_duration_hours']:.1f} å°æ—¶

âœ… æ•°æ®è´¨é‡:
- å¹³å‡å®Œæ•´åº¦: {stats['data_quality']['avg_completeness']:.1%}
- é«˜è´¨é‡è®°å½•: {stats['data_quality']['high_quality_records']:,} æ¡

ğŸ”§ å¤„ç†ç»Ÿè®¡:
- åŸå§‹è®°å½•: {self.total_records:,}
- é‡å¤è®°å½•: {self.duplicate_records:,}
- å”¯ä¸€è®°å½•: {self.unique_records:,}
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
    
    def process_all_data(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®çš„ä¸»å‡½æ•°"""
        print("ğŸš€ ComBaseæ•°æ®å¤„ç†å™¨å¯åŠ¨")
        print("=" * 50)
        
        # 1. æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        data_files = self.find_data_files()
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return None, None
        
        # 2. åŠ è½½å¹¶åˆå¹¶æ•°æ®
        df = self.load_and_merge_data(data_files)
        if df.empty:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return None, None
        
        # 3. å»é™¤é‡å¤
        df = self.remove_duplicates(df)
        
        # 4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        self.validate_data_consistency(df)
        
        # 5. å¢å¼ºæ•°æ®ç»“æ„
        df = self.enhance_data_structure(df)
        
        # 6. ç”Ÿæˆç»Ÿè®¡
        stats = self.generate_summary_statistics(df)
        
        # 7. ä¿å­˜å¤„ç†åçš„æ•°æ®
        files = self.save_processed_data(df, stats)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ!")
        print("=" * 50)
        print(f"å¤„ç†è®°å½•: {len(df):,} æ¡")
        print(f"è¾“å‡ºæ–‡ä»¶: {len(files)} ä¸ª")
        print(f"ä¿å­˜ä½ç½®: {self.processed_dir}")
        
        return df, stats


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ComBaseæ•°æ®å¤„ç†å™¨")
    print("å¤„ç†çˆ¬å–çš„åŸå§‹æ•°æ®ï¼Œæå–å®Œæ•´ä¿¡æ¯å¹¶æ•´ç†å­˜å‚¨")
    print("=" * 60)
    
    try:
        processor = ComBaseDataProcessor()
        df, stats = processor.process_all_data()
        
        if df is not None:
            print(f"\nğŸ“‹ å¤„ç†ç»“æœé¢„è§ˆ:")
            print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            print(f"åˆ—å: {list(df.columns)}")
            
            # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•
            print(f"\nğŸ“Š æ•°æ®æ ·ä¾‹:")
            print(df[['record_id', 'organism', 'food_category', 'food_name', 'temperature', 'logc_points']].head())
            
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
